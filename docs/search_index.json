[
["index.html", "Supplementary materials for paper: Article title here Preface 0.1 Acknowledgments 0.2 Funding", " Supplementary materials for paper: Article title here Robert Z. Selden, Jr.1 2020-06-27 Preface Reproducibility—the ability to recompute results—and replicability—the chances other experimenters will achieve a consistent result—are two foundational characteristics of successful scientific research (Leek and Peng 2015). This volume is written in Markdown, and all files needed to reproduce it are included in the GitHub repository. The reproducible nature of this undertaking provides a means for others to critically assess and evaluate the various analytical components (Gray and Marwick 2019; Peng 2011; Gandrud 2014), which is a necessary requirement for the production of reliable knowledge. Reproducibility projects in psychology and cancer biology are impacting current research practices across all domains. Examples of reproducible research are becoming more abundant in archaeology (Marwick 2016; Ivanovaitė et al. 2020; Selden Jr., Dockall, and Dubied 2020), and the next generation of archaeologists are learning those tools and methods needed to reproduce and/or replicate research results (Marwick et al. 2019). Reproducible and replicable research work flows are often employed at the highest levels of humanities-based inquiries to mitigate concern or doubt regarding proper execution, and is of particular import should the results have—explicitly or implicitly—a major impact on scientific progress (Peels and Bouter 2018). Components of the undertaking presented in this volume are novel, and this basic research endeavour is rooted in archaeological epistemology. Sharing data is not enough, and if investigators share only data absent their analysis code, that places a substantive burden on those who may seek to build upon or replicate their work in the future. In this instance, the choice to pursue a replicable work flow occurs based on the simple fact that this work is foundational, and meant to begin a meaningful discussion related to projectile morphology. As an exploratory research endeavour, this study was not preregistered. 0.1 Acknowledgments This volume enlists a variety of tools from the Open Review Toolkit, as well as the code provided for the bookdown package. I extend my gratitude to all who contribute comments and constructive criticisms throughout the development and maturation of this project. This document will remain in open review until the article is published. 0.2 Funding Components of this analytical work flow were developed and funded by a Preservation Technology and Training grant (P14AP00138) to the author from the National Center for Preservation Technology and Training (NCPTT), and funding for this project was provided by grants to the RZS from the National Forests and Grasslands in Texas (15-PA-11081300-033) and the United States Forest Service (20-PA-11081300-074). References "],
["landmarking-protocol.html", "Chapter 1 Landmarking Protocol 1.1 Generating the spline 1.2 Splitting the spline 1.3 Spline split at location of LM1 1.4 Spline split at locations of LM2 - LM3 1.5 Spline split at locations of LM4 - LM5 1.6 Spline split at location of LM6 1.7 Final spline 1.8 Landmark and semilandmark placement 1.9 Acknowledgments", " Chapter 1 Landmarking Protocol This document includes supplemental materials for the article, “Article title here.” The landmarking protocol outlined here was initially developed for an analysis of Gahagan bifaces (Selden Jr., Dockall, and Dubied 2020). The goals of this study differ; however, the mechanics of the landmarking protocol are similar, and follow components of Dalton projectile morphology identified in previous studies (refs). Geomagic Design X (Build Version 2020.0.1 [Build Number: 30]) was used to generate a spline around the periphery of each biface, and to populate the landmarks and equidistant semilandmarks in a replicable manner using a suite of mathematically-defined criteria. knitr::include_graphics(&#39;images/dalton.png&#39;) fig.cap=&quot;Dalton point KeilMangold13. \\\\label{figdalton}&quot; The goal of this effort was to increase both the precision and rigour of the study by including the z-dimension to capture morphological characteristics associated with axial twisting introduced through the general practices of knapping and beveling. While true that some landmarking protocols can be—and often are—recycled as new specimens are added, this particular research programme endeavours to achieve ever-greater accuracy and precision in each analytical iteration. This landmarking protocol was developed prior to running auto3dgm, and details the procedure for applying landmarks and equidistant semilandmarks; however, it may not reflect their actual placement on this specimen. 1.1 Generating the spline This effort enlists a spline extracted from the surface geometry of the mesh using the extract contour curves command, which is used to detect and extract 3D contour curves from high-curvature areas of the mesh. In reverse-engineering, extract contour curves is regularly employed as the first step in building a patch network that is used to create a surface. The extracted feature curve is rendered as a spline, and follows the highest curvature contours around the periphery of the lateral and basal edges, following the highly variable sinuous edge morphology around the entirety of the projectile. The remainder of the landmarking protocol is based upon this spline, which was subsequently split at six mathematically-defined locations. knitr::include_graphics(&#39;images/extractspline.png&#39;) fig.cap=&quot;Spline extracted along the highest contours of the Dalton point \\\\label{figspline}&quot; 1.2 Splitting the spline A few definitions are warranted before proceeding. Reference geometries are used in the assistance of creating other features. These include basic geometric entities, such as planes, vectors, coordinates, points, and polygons. A reference point is a virtual point and is used to mark a specific position on a model or in 3D space. A reference plane is a virtual plane that has a normal direction and an infinite size. A reference plane is not a surface body, and is used to create other features. The characteristic points and tangents developed for this landmarking protocol were inspired by the work of Birkhoff (1933), which has been gainfully employed within the context of both ceramic (Selden Jr. 2018a, 2018b, 2019, 2020) and lithic analyses (Selden Jr., Dockall, and Shafer 2018; Selden Jr., Dockall, and Dubied 2020). The first landmark (LM1) is placed at the horizontal tangent on the tip of each Dalton point. The second through fifth splits (LM2 - LM5) occur at points of highest curvature, where LM2 is always placed on the right side of the projectile when oriented in 3D space following the alignment output of auto3dgm. To place the final landmark (LM6), a linear measurement was used to project a reference point equidistant between LM2 and LM3. The location of that point was leveraged in placing the reference plane used to cut the spline at the location of LM6. 1.3 Spline split at location of LM1 The horizontal tangent is calculated by drawing a horizontal line above the tip of the biface using the tangent as a common constraint, and the horizontal as the independent constraint. To split the 3D spline at the location of the horizontal tangent, a reference point was inserted at the location of the tangent in the sketch (light blue point; below, left), followed by a reference plane (in white; below, left and right) using the pick point and normal axis function where the reference point (h-tangent) was used as the pick point, and the Right plane as the normal axis (below, left). The spline was then cut at the location where the reference plane intersected with the spline (below image, right). knitr::include_graphics(&#39;images/lm1.png&#39;) fig.cap=&quot;Identify horizontal tangent, insert reference point and reference plane (left). Use reference plane to cut spline at the location of the horizontal tangent (right). \\\\label{figlm1}&quot; 1.4 Spline split at locations of LM2 - LM3 The point of highest curvature on either side of the basal edge was calculated using the curvature function in the Accuracy Analyser. This function displays the curvature flow as a continuous colour plot across the area of the curve. In this instance, curvature is defined as the amount by which a geometric shape deviates from being flat or straight in the case of a line. The curvature is displayed in different colours according to the local radius, and is calculated in only one direction (U or V) along the curve. Using this tool, the two points of highest curvature were located between the basal and lateral edges on either side of each projectile where the local radius measure was largest. The alignment and orientation of each biface was dictated by the auto3dgm output, and the landmarking protocol follows the mesh orientation, where LM2 was always placed on the right side of the basal edge, and LM3 on the left. knitr::include_graphics(&#39;images/splinesplit1.png&#39;) fig.cap=&quot;Identify points of hightest curvature (light blue) at left/right intersection of lateral and basal edges. \\\\label{figsplinesplitlr}&quot; 1.5 Spline split at locations of LM4 - LM5 The point of highest curvature at the intersection of the blade and base was also calculated using the curvature function in the Accuracy Analyser. Using this tool, the two points of highest curvature were located between the blade and base on either side of each projectile where the local radius measure was largest. The alignment and orientation of each biface was dictated by the auto3dgm output, and the landmarking protocol follows the mesh orientation in that figure, where LM4 was always placed on the right side of the basal edge, and LM5 on the left. knitr::include_graphics(&#39;images/splinesplit2.png&#39;) fig.cap=&quot;Identify points of hightest curvature (light blue) at left/right intersection of blade and base. \\\\label{figsplinesplitlr}&quot; 1.6 Spline split at location of LM6 One additional landmark (LM6) was placed at the centre of the base. The location of this landmark was identified by calculating the linear distance between LM2 and LM3, and projecting a reference point (ctrl-div; below) equidistant between the two. A reference plane was added using the ctrl-div as the pick point, and the Right plane as the normal axis. The spline was then split at the intersection of the reference plane and the basal spline. knitr::include_graphics(&#39;images/lm6.png&#39;) fig.cap=&quot;Calculate linear distance between LM2 and LM3, insert reference plane coplanar to Right plane equidistant between LM2 and LM3, and use the reference plane to cut the spline. \\\\label{figlm6}&quot; 1.7 Final spline Through the preceding protocol, the initial spline was split into six discrete splines. These splines articulate with components of projectile morphology that can be compartmentalised in the analyses. The primary analytical gain achieved through this exercise is the requisite foundation needed to carry out replicable analyses of Dalton point morphology in three dimensions, further increasing the precision of the geometric morphometric analysis. knitr::include_graphics(&#39;images/splinesplit-frbl.png&#39;) fig.cap=&quot;Result of spline splits include six discrete splines, each articulating with a region of analytical interest. \\\\label{figsplinesplit-frbl}&quot; 1.8 Landmark and semilandmark placement Landmarks 1 - 6 were placed at the location of each spline split (blue points, below). Equidistant semilandmarks were added to each of the four splines; 10 between LM1 and LM4, three between LM4 and LM2, three between LM2 and LM6, three between LM6 and LM3, three between LM3 and LM5, and 10 between LM5 and LM1. knitr::include_graphics(&#39;images/lmslm-all.png&#39;) fig.cap=&quot;Mesh with landmarks (blue) and equidistant semilandmarks (white) applied. \\\\label{figlmslm-all}&quot; The rigourous protocol used in the application of landmarks and semilandmarks aids in capturing morphological variation that articulates with differing patterns of reduction and axial twisting introduced by knappers through the general practice of knapping, retouch, and beveling. Thus, this constellation of landmarks and semilandmarks provides for greater precision in the geometric morphometric analysis, marking a substantive advancement in those analytical protocols used to achieve a more complete and holistic analysis of Dalton point morphology. 1.9 Acknowledgments I extend my gratitude to Christian S. Hoggard and David K. Thulman for their thoughtful comments and constructive criticisms on the draft of this landmarking protocol, which was originally developed for the study of Gahagan bifaces, and is extended here to an analysis of Dalton point morphology. This iteration of the landmarking protocol was developed using the digit3DLand package in R (code available in Github repository); however, the capacity to populate a replicable suite of reference geometry across the sample in Geomagic Design X made it a better option for the dynamic design process. Definitions of reference geometries and Design X features described in this protocol are paraphrased from the reference manual. References "],
["analysis.html", "Chapter 2 Analysis 2.1 Generalised Procrustes Analysis 2.2 Principal Components Analysis 2.3 Define models 2.4 Allometry 2.5 Size/Shape ~ Group? 2.6 Morphological disparity 2.7 Mean shapes", " Chapter 2 Analysis 2.1 Generalised Procrustes Analysis # library(devtools) # devtools::install_github(&quot;geomorphR/geomorph&quot;, ref = &quot;Stable&quot;, build_vignettes = TRUE) library(geomorph) ## Loading required package: RRPP ## Loading required package: rgl library(wesanderson) setwd(getwd()) # read GM data source(&#39;readmulti.csv.R&#39;) # read .csv files setwd(&quot;./data&quot;) filelist &lt;- list.files(pattern = &quot;.csv&quot;) coords &lt;- readmulti.csv(filelist) setwd(&quot;../&quot;) # read qualitative data qdata &lt;- read.csv(&quot;qdata.csv&quot;,header=TRUE,row.names=1) qdata &lt;- qdata[match(dimnames(coords)[[3]],rownames(qdata)),] qdata ## heart.out heart.reg ## 11AI225 H H ## 11HE445 N P ## HK49_1462 N I ## HK49_2 N I ## HK49_4 N I ## HK49_7 N I ## KeilMangold13 H H ## KeilMangold3 H H ## KeilMangold8 H H ## Kinzer46 N P ## Kinzer49 N P ## Kinzer50 N P # gpa Y.gpa &lt;- gpagen(coords, PrinAxes = TRUE, ProcD = TRUE, Proj = TRUE, print.progress = FALSE) # plot consensus configuration plot(Y.gpa$consensus[,c(&quot;X&quot;, &quot;Y&quot;)], asp = 1, pch = 20) # gpa plot # knitr::include_graphics(&#39;images/gpa3d.png&#39;) # fig.cap=&quot;Results of generalized Procrustes analysis.&quot; # geomorph data frame gdf &lt;- geomorph.data.frame(shape = Y.gpa$coords, size = Y.gpa$Csize, heart = qdata$heart.out, hreg = qdata$heart.reg) # attributes for boxplots csz &lt;- Y.gpa$Csize # centroid size heart &lt;- qdata$heart.out # heartland in/out hreg &lt;- qdata$heart.reg # heartland region # boxplot of Dalton point centroid size by in/out heartland boxplot(csz~heart, names = c(&quot;H&quot;,&quot;N&quot;), # heartland (H), and not heartland (N) xlab = &quot;Heartland&quot;, ylab = &quot;Centroid Size&quot;, col = wes_palette(&quot;Moonrise2&quot;), ) fig.cap = &quot;Boxplot of centroid size by Heartland (in/out).&quot; # boxplot of Dalton point centroid size by heartland + regions boxplot(csz~hreg, names = c(&quot;H&quot;,&quot;I&quot;,&quot;P&quot;), # heartland (H), interior (I), and northern periphery (P) xlab = &quot;Heartland Region&quot;, ylab = &quot;Centroid Size&quot;, col = wes_palette(&quot;Moonrise2&quot;), ) fig.cap = &quot;Boxplot of centroid size by Heartland region.&quot; 2.2 Principal Components Analysis # principal components analysis pca&lt;-gm.prcomp(Y.gpa$coords) summary(pca) ## ## Ordination type: Principal Component Analysis ## Centering and projection: OLS ## Number of observations 12 ## Number of vectors 12 ## ## Importance of Components: ## Comp1 Comp2 Comp3 Comp4 Comp5 ## Eigenvalues 0.009312475 0.00144648 0.0007429635 0.0006407519 0.0003773806 ## Proportion of Variance 0.696908125 0.10824876 0.0556004007 0.0479512917 0.0282416477 ## Cumulative Proportion 0.696908125 0.80515689 0.8607572868 0.9087085785 0.9369502262 ## Comp6 Comp7 Comp8 Comp9 Comp10 ## Eigenvalues 0.0002591519 0.0002263497 0.0001600805 7.788778e-05 6.264734e-05 ## Proportion of Variance 0.0193938872 0.0169391025 0.0119797820 5.828808e-03 4.688275e-03 ## Cumulative Proportion 0.9563441134 0.9732832159 0.9852629978 9.910918e-01 9.957801e-01 ## Comp11 Comp12 ## Eigenvalues 5.638891e-05 2.049974e-33 ## Proportion of Variance 4.219919e-03 1.534118e-31 ## Cumulative Proportion 1.000000e+00 1.000000e+00 # set plot parameters to plot by heartland in (H) and out (N) pch.gps.heart &lt;- c(15,17)[as.factor(heart)] col.gps.heart &lt;- wes_palette(&quot;Moonrise2&quot;)[as.factor(heart)] col.hull &lt;- c(&quot;#798E87&quot;,&quot;#C27D38&quot;) # plot pca by heartland in (H) and out (N) pc.plot1 &lt;- plot(pca, asp = 1, pch = pch.gps.heart, col = col.gps.heart) shapeHulls(pc.plot1, groups = heart, group.cols = col.hull) # set plot parameters to plot by heartland + regions pch.gps.hreg &lt;- c(15,17,18)[as.factor(hreg)] col.gps.hreg &lt;- wes_palette(&quot;Moonrise2&quot;)[as.factor(hreg)] col.hull.2 &lt;- c(&quot;#798E87&quot;,&quot;#CCC591&quot;,&quot;#C27D38&quot;) # plot pca by heartland + regions pc.plot2 &lt;- plot(pca, asp = 1, pch = pch.gps.hreg, col = col.gps.hreg) shapeHulls(pc.plot2, groups = hreg, group.cols = col.hull.2) 2.3 Define models # allometry fit.size &lt;- procD.lm(shape ~ size, data = gdf, print.progress = FALSE, iter = 9999) # allometry - common allometry, different means -&gt; heart.out fit.sz.cheart &lt;- procD.lm(shape ~ size + heart, data = gdf, print.progress = FALSE, iter = 9999) # allometry - unique allometries -&gt; heart.out fit.sz.uheart &lt;- procD.lm(shape ~ size * heart, data = gdf, print.progress = FALSE, iter = 9999) # allometry - common allometry, different means -&gt; heart.reg fit.sz.chreg &lt;- procD.lm(shape ~ size + hreg, data = gdf, print.progress = FALSE, iter = 9999) # allometry - unique allometries -&gt; heart.reg fit.sz.uhreg &lt;- procD.lm(shape ~ size * hreg, data = gdf, print.progress = FALSE, iter = 9999) # size as a function of group? fit.sizeheart &lt;- procD.lm(size ~ heart, data = gdf, print.progress = FALSE, iter = 9999) fit.sizehreg &lt;- procD.lm(size ~ hreg, data = gdf, print.progress = FALSE, iter = 9999) # shape as a function of group? fit.shapeheart &lt;- procD.lm(shape ~ heart, data = gdf, print.progress = FALSE, iter = 9999) fit.shapehreg &lt;- procD.lm(shape ~ hreg, data = gdf, print.progress = FALSE, iter = 9999) 2.4 Allometry # allometry - does shape change with size? anova(fit.size) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## size 1 0.058354 0.058354 0.397 6.5837 2.6047 0.0056 ** ## Residuals 10 0.088634 0.008863 0.603 ## Total 11 0.146988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ size, iter = 9999, data = gdf, print.progress = FALSE) # heart.out anova(fit.sz.cheart) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## size 1 0.058354 0.058354 0.39700 7.3641 2.7129 0.0036 ** ## heart 1 0.017317 0.017317 0.11781 2.1854 1.3260 0.1107 ## Residuals 9 0.071317 0.007924 0.48519 ## Total 11 0.146988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ size + heart, iter = 9999, data = gdf, print.progress = FALSE) anova(fit.sz.uheart) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## size 1 0.058354 0.058354 0.39700 8.1258 2.7877 0.0027 ** ## heart 1 0.017317 0.017317 0.11781 2.4114 1.4502 0.0930 . ## size:heart 1 0.013867 0.013867 0.09434 1.9310 1.1401 0.1419 ## Residuals 8 0.057450 0.007181 0.39085 ## Total 11 0.146988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ size * heart, iter = 9999, data = gdf, print.progress = FALSE) anova(fit.sz.cheart, fit.sz.uheart, print.progress = FALSE) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Effect sizes (Z) based on F distributions ## ## ResDf Df RSS SS MS Rsq F Z ## shape ~ size + heart (Null) 9 1 0.071317 0.00000 ## shape ~ size * heart 8 1 0.057450 0.013867 0.013867 0.09434 1.931 1.1401 ## Total 11 0.146988 ## P Pr(&gt;F) ## shape ~ size + heart (Null) ## shape ~ size * heart 0.1419 ## Total # heart.reg anova(fit.sz.chreg) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## size 1 0.058354 0.058354 0.39700 7.0962 2.65162 0.0048 ** ## hreg 2 0.022849 0.011424 0.15544 1.3893 0.67602 0.2522 ## Residuals 8 0.065786 0.008223 0.44756 ## Total 11 0.146988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ size + hreg, iter = 9999, data = gdf, print.progress = FALSE) anova(fit.sz.uhreg) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## size 1 0.058354 0.058354 0.39700 7.2700 2.58801 0.0058 ** ## hreg 2 0.022849 0.011424 0.15544 1.4233 0.68139 0.2490 ## size:hreg 2 0.017626 0.008813 0.11991 1.0980 0.27407 0.3773 ## Residuals 6 0.048160 0.008027 0.32764 ## Total 11 0.146988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ size * hreg, iter = 9999, data = gdf, print.progress = FALSE) anova(fit.sz.chreg, fit.sz.uhreg, print.progress = FALSE) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Effect sizes (Z) based on F distributions ## ## ResDf Df RSS SS MS Rsq F Z ## shape ~ size + hreg (Null) 8 1 0.065786 0.00000 ## shape ~ size * hreg 6 2 0.048160 0.017626 0.008813 0.11991 1.098 0.27407 ## Total 11 0.146988 ## P Pr(&gt;F) ## shape ~ size + hreg (Null) ## shape ~ size * hreg 0.3773 ## Total # allometry plots # regscore (Drake and Klingenberg 2008) plot(fit.size, type = &quot;regression&quot;, reg.type = &quot;RegScore&quot;, predictor = log(gdf$size), pch = pch.gps.heart, col = col.gps.heart) plot(fit.size, type = &quot;regression&quot;, reg.type = &quot;RegScore&quot;, predictor = log(gdf$size), pch = pch.gps.hreg, col = col.gps.hreg) # common allometric component (Mitteroecker 2004) plotAllometry(fit.size, size = gdf$size, logsz = TRUE, method = &quot;CAC&quot;, pch = pch.gps.heart, col = col.gps.heart) plotAllometry(fit.size, size = gdf$size, logsz = TRUE, method = &quot;CAC&quot;, pch = pch.gps.hreg, col = col.gps.hreg) # size-shape pca (Mitteroecker 2004) plotAllometry(fit.size, size = gdf$size, logsz = TRUE, method = &quot;size.shape&quot;, pch = pch.gps.heart, col = col.gps.heart) plotAllometry(fit.size, size = gdf$size, logsz = TRUE, method = &quot;size.shape&quot;, pch = pch.gps.hreg, col = col.gps.hreg) # predline (Adams and Nistri 2010) plotAllometry(fit.sz.uheart, size = gdf$size, logsz = TRUE, method = &quot;PredLine&quot;, pch = pch.gps.heart, col = col.gps.heart) plotAllometry(fit.sz.uhreg, size = gdf$size, logsz = TRUE, method = &quot;PredLine&quot;, pch = pch.gps.hreg, col = col.gps.hreg) 2.5 Size/Shape ~ Group? # ANOVA: does Dalton point size differ in/out of heartland? anova(fit.sizeheart) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## heart 1 1439.8 1439.75 0.24911 3.3175 1.0579 0.09605 . ## Residuals 10 4339.9 433.99 0.75089 ## Total 11 5779.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = size ~ heart, iter = 9999, data = gdf, print.progress = FALSE) # ANOVA: does Dalton point morphology differ in/out of Heartland? anova(fit.shapeheart) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## heart 1 0.04698 0.046980 0.31962 4.6977 2.1383 0.0213 * ## Residuals 10 0.10001 0.010001 0.68038 ## Total 11 0.14699 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ heart, iter = 9999, data = gdf, print.progress = FALSE) # ANOVA: does Dalton point sizes differ across heartland regions? anova(fit.sizehreg) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## hreg 2 1475.3 737.63 0.25525 1.5423 0.65995 0.2554 ## Residuals 9 4304.4 478.27 0.74475 ## Total 11 5779.7 ## ## Call: procD.lm(f1 = size ~ hreg, iter = 9999, data = gdf, print.progress = FALSE) # pairwise comparison of LS means = which differ? sz.hreg &lt;- pairwise(fit.sizehreg, groups = qdata$heart.reg) summary(sz.hreg, confidence = 0.95, test.type = &quot;dist&quot;) ## ## Pairwise comparisons ## ## Groups: H I P ## ## RRPP: 10000 permutations ## ## LS means: ## Vectors hidden (use show.vectors = TRUE to view) ## ## Pairwise distances between means, plus statistics ## d UCL (95%) Z Pr &gt; d ## H:I 21.129079 31.37133 0.8383642 0.2068 ## H:P 25.342765 31.14089 1.2981949 0.1210 ## I:P 4.213687 30.91455 -0.9357657 0.7977 # pairwise distance between variances = standardization? summary(sz.hreg, confidence = 0.95, test.type = &quot;var&quot;) ## ## Pairwise comparisons ## ## Groups: H I P ## ## RRPP: 10000 permutations ## ## ## Observed variances by group ## ## H I P ## 278.3626 338.1850 459.5544 ## ## Pairwise distances between variances, plus statistics ## d UCL (95%) Z Pr &gt; d ## H:I 59.82237 539.7653 -1.0838680 0.8549 ## H:P 181.19176 537.6419 -0.3412214 0.5737 ## I:P 121.36939 536.7711 -0.7061467 0.7036 # ANOVA: does Dalton point morphology differ across heartland + regions? anova(fit.shapehreg) ## ## Analysis of Variance, using Residual Randomization ## Permutation procedure: Randomization of null model residuals ## Number of permutations: 10000 ## Estimation method: Ordinary Least Squares ## Sums of Squares and Cross-products: Type I ## Effect sizes (Z) based on F distributions ## ## Df SS MS Rsq F Z Pr(&gt;F) ## hreg 2 0.052684 0.026342 0.35842 2.514 1.5203 0.07175 . ## Residuals 9 0.094304 0.010478 0.64158 ## Total 11 0.146988 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Call: procD.lm(f1 = shape ~ hreg, iter = 9999, data = gdf, print.progress = FALSE) # pairwise comparison of LS means = which differ? sh.hreg &lt;- pairwise(fit.shapehreg, groups = qdata$heart.reg) summary(sh.hreg, confidence = 0.95, test.type = &quot;dist&quot;) ## ## Pairwise comparisons ## ## Groups: H I P ## ## RRPP: 10000 permutations ## ## LS means: ## Vectors hidden (use show.vectors = TRUE to view) ## ## Pairwise distances between means, plus statistics ## d UCL (95%) Z Pr &gt; d ## H:I 0.12340118 0.1390142 1.5238546 0.0951 ## H:P 0.14640107 0.1363220 2.3425328 0.0303 ## I:P 0.05340121 0.1368627 -0.7453086 0.7464 # pairwise distance between variances = standardization? summary(sh.hreg, confidence = 0.95, test.type = &quot;var&quot;) ## ## Pairwise comparisons ## ## Groups: H I P ## ## RRPP: 10000 permutations ## ## ## Observed variances by group ## ## H I P ## 0.013484388 0.003353103 0.006738604 ## ## Pairwise distances between variances, plus statistics ## d UCL (95%) Z Pr &gt; d ## H:I 0.010131285 0.01094240 1.6093311 0.0739 ## H:P 0.006745784 0.01093859 0.5395385 0.2904 ## I:P 0.003385501 0.01090853 -0.5006636 0.6352 2.6 Morphological disparity # morphological disparity: does Dalton point size display greater shape variation among individuals? # in/out of heartland morphol.disparity(fit.sizeheart, groups = qdata$heart.out, data = gdf, print.progress = FALSE, iter = 9999) ## ## Call: ## morphol.disparity(f1 = fit.sizeheart, groups = qdata$heart.out, ## iter = 9999, data = gdf, print.progress = FALSE) ## ## ## ## Randomized Residual Permutation Procedure Used ## 10000 Permutations ## ## Procrustes variances for defined groups ## H N ## 278.3626 403.3085 ## ## ## Pairwise absolute differences between variances ## H N ## H 0.0000 124.9459 ## N 124.9459 0.0000 ## ## ## P-Values ## H N ## H 1.0000 0.6021 ## N 0.6021 1.0000 # heartland + regions morphol.disparity(fit.sizehreg, groups = qdata$heart.reg, data = gdf, print.progress = FALSE, iter = 9999) ## ## Call: ## morphol.disparity(f1 = fit.sizehreg, groups = qdata$heart.reg, ## iter = 9999, data = gdf, print.progress = FALSE) ## ## ## ## Randomized Residual Permutation Procedure Used ## 10000 Permutations ## ## Procrustes variances for defined groups ## H I P ## 278.3626 338.1850 459.5544 ## ## ## Pairwise absolute differences between variances ## H I P ## H 0.00000 59.82237 181.1918 ## I 59.82237 0.00000 121.3694 ## P 181.19176 121.36939 0.0000 ## ## ## P-Values ## H I P ## H 1.0000 0.8549 0.5737 ## I 0.8549 1.0000 0.7036 ## P 0.5737 0.7036 1.0000 # morphological disparity: does Dalton point morphology display greater shape variation among individuals? # in/out of heartland morphol.disparity(fit.shapeheart, groups = qdata$heart.out, data = gdf, print.progress = FALSE, iter = 9999) ## ## Call: ## morphol.disparity(f1 = fit.shapeheart, groups = qdata$heart.out, ## iter = 9999, data = gdf, print.progress = FALSE) ## ## ## ## Randomized Residual Permutation Procedure Used ## 10000 Permutations ## ## Procrustes variances for defined groups ## H N ## 0.013484388 0.005758776 ## ## ## Pairwise absolute differences between variances ## H N ## H 0.000000000 0.007725612 ## N 0.007725612 0.000000000 ## ## ## P-Values ## H N ## H 1.0000 0.1013 ## N 0.1013 1.0000 # heartland + regions morphol.disparity(fit.shapehreg, groups = qdata$heart.reg, data = gdf, print.progress = FALSE, iter = 9999) ## ## Call: ## morphol.disparity(f1 = fit.shapehreg, groups = qdata$heart.reg, ## iter = 9999, data = gdf, print.progress = FALSE) ## ## ## ## Randomized Residual Permutation Procedure Used ## 10000 Permutations ## ## Procrustes variances for defined groups ## H I P ## 0.013484388 0.003353103 0.006738604 ## ## ## Pairwise absolute differences between variances ## H I P ## H 0.000000000 0.010131285 0.006745784 ## I 0.010131285 0.000000000 0.003385501 ## P 0.006745784 0.003385501 0.000000000 ## ## ## P-Values ## H I P ## H 1.0000 0.0739 0.2904 ## I 0.0739 1.0000 0.6352 ## P 0.2904 0.6352 1.0000 2.7 Mean shapes # subset landmark coordinates to produce mean shapes by site # new.coords &lt;- coords.subset(A = Y.gpa$coords, group = qdata$heart) # names(new.coords) # group shape means # mean &lt;- lapply(new.coords, mshape) # plot(mean$vv) # mean shapes #knitr::include_graphics(&#39;images/dalton-mshape.png&#39;) #fig.cap = &quot;Mean shapes for Dalton points by region&quot; # end of code "],
["open.html", "Chapter 3 Open Review 3.1 FAQ about open review 3.2 Privacy and Consent Policy", " Chapter 3 Open Review Open Review means that you can freely read this volume and easily help to make it better. You can offer suggestions by making annotations using hypothes.is, an open source annotation system. This is a simple system for interacting with this volume. If you are familiar with GitHub or Git, you can also comment using GitHub’s issue tracker for the volume, or make a pull request. In addition to these feedback options, this website for the book will be collecting your implicit feedback by tracking the readership and abandonment rate of each section. Open Review takes place before and during the peer review process. The feedback from Open Review and peer review will be used to create a revised manuscript. The Open Review period will end when the final manuscript is submitted to the publisher. The concept of Open Review, as implemented here, is taken from Matthew Salganik’s Open Review Toolkit. Much of the text on this page comes from the Open Review Toolkit About page and the Open Review Toolkit Privacy and Consent page 3.1 FAQ about open review 3.1.1 What kind of feedback are you looking for? Open Review is not just about catching typos. Rather, Open Review is designed to collect all types of feedback, and I would particularly welcome feedback that relates to the substance of the volume. Are there sections that you find particularly confusing? Are their points that you find particularly important? Am I making claims that you think need to be refined? Are there parts of the book that you think should be removed? When in doubt, I ask that you follow one of the main principles at Wikipedia: Be bold. 3.1.2 Can I see the annotations that others are making? Yes, all annotations are public. You can view them on right side of the each page. 3.1.3 What are the benefits for readers? You get to read the manuscript and participate in making it better. 3.1.4 What are the benefits for authors and publishers? The Open Review process will benefit both authors and publishers, even if they have no interest in increasing access to knowledge. The process will lead to higher manuscript quality through the explicit and implicit feedback. Further, the Open Review process will provide valuable data that can be used during that marketing of the book. 3.1.5 Has anyone ever done something like this before? This is based on Matthew Salganik’s Open Review Toolkit. He has written about some related efforts here: http://www.bitbybitbook.com/en/open-review/ 3.1.6 What kind of information are you collecting and how will that information be used? Please read the privacy and consent policy, below. 3.1.7 How I can learn more about traditional peer review of academic books? The AAUP recently published a report on best practices for peer review. 3.1.8 Can I do this with my book? Absolutely. View the code for this website at https://github.com/aksel-blaise/dalton for more about how we did it. 3.1.9 I have a different question about Open Review. How can I get in touch? Send an email to zselden@sfasu.edu 3.2 Privacy and Consent Policy 3.2.1 Overview The complete text of the supplementary data is made available on this website at no cost. While you are reading the volume, reader behavior is measured in aggregate. For example, which sections of the book get read most often is measured. These data will help to improve the book. 3.2.2 What information do we collect? Google Analytics is used to collect information about how you interact with this website. Further, like most websites, cookies are used to enhance your experience, gather general visitor information, and track visits to the website. Please refer to the “do we use cookies?” section below for information about cookies and how we use them. 3.2.3 How do we use your information? Any of the information that we collect may be used for research, to improve the supplementary data. 3.2.4 How do we protect your information? A variety of security measures is used to maintain the safety of the information that you provide. Most of the browsing information that we gather is stored in Google Analytics, and you can read more about their security and privacy principles. Annotations that you can add are managed by hypothes.is, and you can read more about their terms of service. Our website is hosted by Github Pages, and you can read more about Github’s terms of service. 3.2.5 Do we use cookies? Yes. Cookies are small files that a site or its service provider transfers to your computer’s hard drive through your web browser (if you allow) that enables the sites or service providers systems to recognize your browser and capture and remember certain information. In order to offer you a better site experience, cookies are used to understand and save your preferences for future visits, and to compile aggregate data regarding site traffic. 3.2.6 Your consent By using this site, you consent to the privacy policy. 3.2.7 Questions If you have any questions, please contact me by email at zselden@sfasu.edu 3.2.8 Changes to our Privacy and Consent Policy I reserve the right to this privacy policy from time to time at my discretion. Please periodically check this section to review the current version of the Privacy and Consent Policy. All previous policies are included in the associated repository, and this page will be updated if any changes are needed. 3.2.9 Acknowledgments The bulk of the (modified) text above comes from Ben Marwick’s bookdown example that illustrates how each of the pieces of this volume function independently, and as a whole. Many thanks to Ben for making this available, and I encourage other archaeologists to make use of these tools. "],
["references.html", "References", " References "]
]
